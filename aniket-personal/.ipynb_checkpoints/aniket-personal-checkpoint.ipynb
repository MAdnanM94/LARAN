{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6caaecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp38-cp38-manylinux1_x86_64.whl (371 kB)\n",
      "\u001b[K     |████████████████████████████████| 371 kB 664 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/aniket/anaconda3/lib/python3.8/site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: pillow in /home/aniket/anaconda3/lib/python3.8/site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/aniket/.local/lib/python3.8/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/aniket/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/aniket/.local/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/aniket/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aniket/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in /home/aniket/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdaf66a",
   "metadata": {},
   "source": [
    "We will first read the .csv files for the first and the second Presidential debate of 2020. Then we will combine these files by merging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>election_year</th>\n",
       "      <th>date</th>\n",
       "      <th>candidate</th>\n",
       "      <th>result</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>John Kennedy</td>\n",
       "      <td>I uh – said that Ive served this country for f...</td>\n",
       "      <td>Pres</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>John Kennedy</td>\n",
       "      <td>Mr. Howe, Mr. Vice President. First uh – let m...</td>\n",
       "      <td>Pres</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Richard Nixon</td>\n",
       "      <td>Mr. Howe, Senator Kennedy, my fellow Americans...</td>\n",
       "      <td>Pres</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>John Kennedy</td>\n",
       "      <td>Good evening, Mr. Howe.</td>\n",
       "      <td>Pres</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Richard Nixon</td>\n",
       "      <td>Good evening, Mr. Howe.</td>\n",
       "      <td>Pres</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        speaker  \\\n",
       "0           0   John Kennedy   \n",
       "1           1   John Kennedy   \n",
       "2           2  Richard Nixon   \n",
       "3           3   John Kennedy   \n",
       "4           4  Richard Nixon   \n",
       "\n",
       "                                                text  type  election_year  \\\n",
       "0  I uh – said that Ive served this country for f...  Pres         1960.0   \n",
       "1  Mr. Howe, Mr. Vice President. First uh – let m...  Pres         1960.0   \n",
       "2  Mr. Howe, Senator Kennedy, my fellow Americans...  Pres         1960.0   \n",
       "3                            Good evening, Mr. Howe.  Pres         1960.0   \n",
       "4                            Good evening, Mr. Howe.  Pres         1960.0   \n",
       "\n",
       "         date  candidate  result party  \n",
       "0  1960-10-21        1.0       1     D  \n",
       "1  1960-10-21        1.0       1     D  \n",
       "2  1960-10-21        1.0       0     R  \n",
       "3  1960-10-21        1.0       1     D  \n",
       "4  1960-10-21        1.0       0     R  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "path = '/Users/nikhil/data/ML_examples/erdos/'\n",
    "\n",
    "speech_data = pd.read_csv('finaldata_party.csv')\n",
    "speech_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeb5c4",
   "metadata": {},
   "source": [
    "Perform a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b66f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speech,test_speech = train_test_split(speech_data.copy(),test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075011ca",
   "metadata": {},
   "source": [
    "Lets define the column transformer class. The column transformer will select only those rows from the data frame where the speaker is Joe Biden and Donald Trump (We will ignore text from Chris Wallace as he is the mediator). The column transformer will then add a target column to the data frame. The value in the target column for rows with speaker as Joe Biden  will be 1 and the value in the target column for rows with speaker as Donald Trump will be 0 (1 indicating that the candiddate got elected as the President and 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47692227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,min_date,max_date,type):\n",
    "        '''\n",
    "        Inputs: \n",
    "        min_date: Select the start date, format: \"YYYY-MM-DD\",(if None selects all dates)\n",
    "        max_date: Select the end date, format: \"YYYY-MM-DD\",(if None selects all dates)\n",
    "        type: List of debate types, e.g ['VP','Pres','Dem','Rep'],(if None all types will be considered)\n",
    "        \n",
    "        Output:\n",
    "        Debate Date, Debate Type selected data frame which contains only the 'Democratic' and 'Republican' party debates\n",
    "        with targets 0 for 'Republican' and 1 for 'Democratic'\n",
    "        '''\n",
    "        self.min_date = min_date\n",
    "        self.max_date = max_date\n",
    "        self.type = type\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if self.min_date!=None and self.max_date!=None:\n",
    "            X['date'] = pd.to_datetime(X['date'])\n",
    "            date_mask = (X['date'] > self.min_date) & (X['date'] <= self.max_date)\n",
    "            masked_data = X.loc[date_mask]\n",
    "        else:\n",
    "            masked_data = X\n",
    "    \n",
    "        if self.type!=None:\n",
    "            masked1_data = masked_data.loc[masked_data['type'].isin(self.type)]\n",
    "        else:\n",
    "            masked1_data = masked_data\n",
    "        masked2_data = masked1_data.loc[masked_data['party'].isin(['R','D'])]\n",
    "        masked2_data.loc[masked2_data['party'] == 'R', 'party'] = 0\n",
    "        masked2_data.loc[masked2_data['party'] == 'D', 'party'] = 1\n",
    "        masked2_data['party'] = masked2_data['party'].astype(int)\n",
    "        return masked2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d04386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set True for bigrams otherwise set False for unigrams\n",
    "Transformer = Custom_Transformer(min_date=None,max_date=None,type=None)\n",
    "\n",
    "df_new_train = Transformer.fit_transform(train_speech)\n",
    "df_new_test =  Transformer.fit_transform(test_speech)\n",
    "print(df_new_train['speaker'].unique())\n",
    "df_new_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31d2be",
   "metadata": {},
   "source": [
    "Define the TF-IDF vectorizer by using ngrams = (1,1) for unigrams and ngrams = (2,2) for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea690ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = False\n",
    "if bigrams:\n",
    "   ngrams = (2,2)\n",
    "else:\n",
    "   ngrams = (1,1)\n",
    "\n",
    "Tfidf = sk_text.TfidfVectorizer(max_features=5000,lowercase=True,analyzer='word',stop_words='english',ngram_range=ngrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b7f54",
   "metadata": {},
   "source": [
    "Separate the X and y for training and test data set. X is the 'text' column and y is the 'target' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_new_train['text']\n",
    "y = df_new_train['party']\n",
    "X_test = df_new_test['text']\n",
    "y_test = df_new_test['party']\n",
    "Tfidf.fit_transform(df_new_train['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3385287",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c8a368",
   "metadata": {},
   "source": [
    "Lets generate a pipeline called Voting_classifier. The pipeline will initially convert the 'text' column from the input data to a sparse matrix of words using the TF-IDF vectorizer and feed that sparse matrix to sklearn's Multinomial Naive Bayes estimator. The Pipeline function is very useful when we are dealing with huge NLP data sets which require a lot of preprocessing before feeding it to the model. The Pipeline function can also be used for hyper-parametrer tuning using MyGridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_classifier = Pipeline([(\"Tfidf vectorizer\",Tfidf),(\"Multinomial NB\",MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ac48a",
   "metadata": {},
   "source": [
    "Fit the Voting_classifier to the train data and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting_classifier.fit(X,y)\n",
    "y_predict = Voting_classifier.predict(X_test)\n",
    "y_predict_train = Voting_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabe2a1",
   "metadata": {},
   "source": [
    "Some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a345b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Voting_classifier, X_test, y_test)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a79a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Voting_classifier, X, y)\n",
    "print(classification_report(y, y_predict_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44b707",
   "metadata": {},
   "source": [
    "During training, the multinomial naive bayes calculates probabilities such as $Pr(\\textrm{'Great'}\\ |\\ \\textrm{Win}),$ the probability that the word \"Great\" appears in the candidate's speech, given that the candidate has won the election.  Using these probabilities, we can define a **polarity score** for each word $w$,\n",
    "\n",
    "$$\\textrm{polarity}(w) = \\log\\left(\\frac{Pr(w\\ |\\ \\textrm{won})}{Pr(w\\ |\\ \\textrm{lost})}\\right).$$\n",
    "\n",
    "Polarity analysis is an example where a simpler model (naive Bayes) offers more explicability than more complicated models.  Aside from this, naive Bayes models are easy to train, the training process is parallelizable, and these models lend themselves well to online learning.  Given enough training data, naive Bayes models have performed well in NLP applications such as spam filtering.\n",
    "\n",
    "Speaking in terms of python code, the command  \"**voting_classifier['Tfidf vectorizer'].get_feature_names_out()**\"  will give you a list of filtered words that were used as features in the multinomial naive bayes method. We can get **$\\log(Pr(w\\ |\\ \\textrm{won}))$** and **$\\log(Pr(w\\ |\\ \\textrm{lost}))$** by using the sklearn's \"**model.feature_log_prob**\" command as follows,\n",
    "\n",
    "$\\log(Pr(w\\ |\\ \\textrm{won}))$ = classifier['Multinomial NB'].feature_log_prob_[1]\n",
    "\n",
    "$\\log(Pr(w\\ |\\ \\textrm{lost}))$ = classifier['Multinomial NB'].feature_log_prob_[0]\n",
    "\n",
    "Using this, we can calculate the polarity score for all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c7990",
   "metadata": {},
   "source": [
    "Some of the words that the TF-IDF vectorizer uses as features are given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Voting_classifier['Tfidf vectorizer'].get_feature_names_out()\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polar_words(classifier,top):\n",
    "    coeff_pos = classifier['Multinomial NB'].feature_log_prob_[1]\n",
    "    coeff_neg = classifier['Multinomial NB'].feature_log_prob_[0]\n",
    "    words = classifier['Tfidf vectorizer'].get_feature_names_out()\n",
    "    words_join=[]\n",
    "    for w in words:\n",
    "        words_join.append(w.replace(\" \", \"_\"))\n",
    "    words_join = np.array(words_join)\n",
    "    polarity = coeff_pos-coeff_neg\n",
    "    polarity_sorted = np.sort(polarity)\n",
    "    index_sort = np.argsort(polarity)\n",
    "    words_sorted = words_join[index_sort]\n",
    "    words_positive = words_sorted[-top:]\n",
    "    words_negative = words_sorted[:top]\n",
    "    wc1 = WordCloud(width = 1000, height = 500,background_color=\"white\").generate(\" \".join(words_positive))\n",
    "    wc2 = WordCloud(width = 1000, height = 500,background_color=\"white\").generate(\" \".join(words_negative))\n",
    "    return wc1,wc2,words_join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f4eee",
   "metadata": {},
   "source": [
    "Lets plot the top 50 most polar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc1,wc2,feature_names = get_polar_words(Voting_classifier,50)\n",
    "\n",
    "f = plt.figure(figsize=(150,50))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "ax1.imshow(wc1)\n",
    "ax1.set_xlabel('Democratic',fontsize=250)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(wc2)\n",
    "ax2.set_xlabel('Republican',fontsize=250)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-white",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
